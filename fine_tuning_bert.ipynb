{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davies-w/colab-github/blob/main/fine_tuning_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install required Libraries"
      ],
      "metadata": {
        "id": "2EuMRx8rm8pb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch]"
      ],
      "metadata": {
        "id": "Ju3_22jOE1qC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05e57a98-f1e3-49b4-af96-beb517bac973"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu121)\n",
            "Collecting accelerate>=0.20.3 (from transformers[torch])\n",
            "  Downloading accelerate-0.26.1-py3-none-any.whl (270 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.26.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from transformers import TrainingArguments, Trainer, AutoTokenizer, AutoModelForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "FPP8aoN8_cW3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "jjozQzYBSm5R",
        "outputId": "efd43764-744f-4bdb-8107-0036bd2f8e56"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    127\u001b[0m   )\n\u001b[1;32m    128\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/drive/MyDrive/dataset_day3/Archive (15).zip'"
      ],
      "metadata": {
        "id": "DCUXBHRPSmye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the Dataset"
      ],
      "metadata": {
        "id": "5s-E8m9onDsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\n",
        "    \"train_set.csv\",\n",
        ")\n",
        "df.shape"
      ],
      "metadata": {
        "id": "cc1joDlBibqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "dfi-n1LEECfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jwn3XTUfTS5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding the Citatinos to the original text"
      ],
      "metadata": {
        "id": "kuPS7tgknB8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Text']= df['Text'] + df['Citations']"
      ],
      "metadata": {
        "id": "T5BEkST1ywH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_split = 0.1\n",
        "\n",
        "# Initial train and test split.\n",
        "train_df, test_df = train_test_split(\n",
        "    df,\n",
        "    test_size=test_split,\n",
        ")\n",
        "\n",
        "\n",
        "print(f\"Number of rows in training set: {len(train_df)}\")\n",
        "print(f\"Number of rows in test set: {len(test_df)}\")"
      ],
      "metadata": {
        "id": "YG1EvQjeOGd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we prepare the text and labels for the Train and Test Dataset"
      ],
      "metadata": {
        "id": "mt_iyVZpnOYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a list of columns that should not be chosen as label columns\n",
        "not_chosen_columns = ['CELEX_ID', 'Text','Citations']\n",
        "\n",
        "# Select label columns that are not in the list of not chosen columns\n",
        "label_columns = [col for col in df.columns if col not in not_chosen_columns]\n",
        "\n",
        "# Create a new DataFrame containing only the selected label columns\n",
        "df_labels_train = train_df[label_columns]\n",
        "df_labels_test = test_df[label_columns]\n",
        "\n",
        "\n",
        "# Convert the label columns to lists for each row\n",
        "labels_list_train = df_labels_train.values.tolist()\n",
        "labels_list_test = df_labels_test.values.tolist()\n",
        "\n"
      ],
      "metadata": {
        "id": "O2oUMBL7HJAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting the integer lists inside labels_list to lists of floats"
      ],
      "metadata": {
        "id": "3Ou3gMXKnen0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert integer lists inside labels_list to lists of floats\n",
        "labels_list_train = [[float(label) for label in labels] for labels in labels_list_train]\n",
        "labels_list_test = [[float(label) for label in labels] for labels in labels_list_test]\n",
        "\n"
      ],
      "metadata": {
        "id": "WzzIS15SIuq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "QJO2KkV8nvTv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-tuning a pretrained BERT Model on our multi label problem"
      ],
      "metadata": {
        "id": "GlNbTvTpn02n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## TRAIN\n",
        "\n",
        "train_texts = train_df['Text'].tolist()\n",
        "train_labels = labels_list_train\n",
        "\n",
        "eval_texts = test_df['Text'].tolist()\n",
        "eval_labels = labels_list_test\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "train_encodings = tokenizer(train_texts, padding=\"max_length\", truncation=True, max_length=512)\n",
        "eval_encodings = tokenizer(eval_texts, padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "\n",
        "class TextClassifierDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "train_dataset = TextClassifierDataset(train_encodings, train_labels)\n",
        "eval_dataset = TextClassifierDataset(eval_encodings, eval_labels)\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    problem_type=\"multi_label_classification\",\n",
        "    num_labels=93\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "jhzl4solDjok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting the training options"
      ],
      "metadata": {
        "id": "IphFTw9OoEga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## TRAIN\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=\".\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=8,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_arguments,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "Gik7DrEpvFjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sumission"
      ],
      "metadata": {
        "id": "g9aJmpFLoKBS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize the submission dataset, put the model on evaluation model and send to GPU if available"
      ],
      "metadata": {
        "id": "l1zYerFSoNR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## SUB\n",
        "sub_encodings = tokenizer(sub_texts, padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "id": "ihUmtx99GgaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the preidction tensor for the values"
      ],
      "metadata": {
        "id": "-tJfKjZTodaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### SAVE ALL PREDICTIONS\n",
        "\n",
        "# List of texts to process\n",
        "text_list = sub_df['Text'].tolist()\n",
        "\n",
        "# Initialize an empty list to store probabilities\n",
        "all_probabilities = []\n",
        "\n",
        "\n",
        "\n",
        "# Process each text\n",
        "for text in text_list:\n",
        "    inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    inputs = inputs.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    probabilities = torch.sigmoid(logits)  # Apply sigmoid to convert logits to probabilities\n",
        "\n",
        "    all_probabilities.append(probabilities)\n",
        "\n",
        "# Now all_probabilities is a list of tensors, where each tensor contains the probabilities for a text\n",
        "\n",
        "# You can convert all_probabilities to a NumPy array if needed\n",
        "all_probabilities_array = np.array([tensor.cpu().numpy() for tensor in all_probabilities])\n",
        "\n",
        "# You can save all_probabilities_array to a file using NumPy's save function\n",
        "np.save(\"all_probabilities.npy\", all_probabilities_array)"
      ],
      "metadata": {
        "id": "fn8NQu_0qER3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take the labels with probabilites More than 0.2. (or the max if nothing is available)"
      ],
      "metadata": {
        "id": "NMcQugJkonJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved probabilities\n",
        "all_probabilities_array = np.load(\"all_probabilities.npy\")\n",
        "\n",
        "# Define the new threshold (e.g., 0.4)\n",
        "new_threshold = 0.2\n",
        "\n",
        "# Create an empty DataFrame with the correct columns\n",
        "df_submission = pd.DataFrame(columns=label_columns)\n",
        "\n",
        "# Iterate through the rows of all_probabilities_array\n",
        "for row in all_probabilities_array:\n",
        "    # Create a mask for values greater than or equal to the new threshold\n",
        "    mask = row >= new_threshold\n",
        "\n",
        "    # Check if any value is above the threshold\n",
        "    if mask.any():\n",
        "        # If there is at least one value above the threshold, use the mask\n",
        "        thresholded_row = mask.astype(int)\n",
        "    else:\n",
        "        # If there are no values above the threshold, take the top 1 value\n",
        "        top_1_index = np.argmax(row)\n",
        "        thresholded_row = np.zeros(len(label_columns), dtype=int)\n",
        "        thresholded_row[top_1_index] = 1\n",
        "\n",
        "    # Reshape the thresholded_row to match the length of the index\n",
        "    thresholded_row = thresholded_row.reshape(1, -1)\n",
        "\n",
        "    # Append the thresholded row to the DataFrame\n",
        "    df_submission = df_submission.append(pd.DataFrame(thresholded_row, columns=label_columns), ignore_index=True)\n",
        "\n",
        "# Convert the DataFrame to integer type\n",
        "df_submission = df_submission.astype(int)\n"
      ],
      "metadata": {
        "id": "Az6-Jqq5rClv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot how many rows with 1,2 or 3 labels"
      ],
      "metadata": {
        "id": "lyl2lrumovGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the sum of values in each row\n",
        "row_sums = df_submission.sum(axis=1)\n",
        "\n",
        "# Plot a histogram of the row sums\n",
        "plt.hist(row_sums, bins=20, edgecolor='k')\n",
        "plt.xlabel('Sum of Row Values')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Row Sums')\n",
        "\n",
        "# Add text labels with the number of values in each bin\n",
        "n, bins, patches = plt.hist(row_sums, bins=20, edgecolor='k')\n",
        "for i in range(len(patches)):\n",
        "    plt.text(bins[i], n[i], str(int(n[i])), ha='center', va='bottom')\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V_4ySSLTtL65"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}